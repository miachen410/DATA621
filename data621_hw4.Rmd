---
title: "DATA621 - HW#4"
author: "Mia Chen, Wei Zhou"
date: "4/26/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Overview

In this homework assignment, you will explore, analyze and model a data set containing approximately 8000 records representing a customer at an auto insurance company. Each record has two response variables. The first response variable, TARGET_FLAG, is a 1 or a 0. A “1” means that the person was in a car crash. A zero means that the person was not in a car crash. The second response variable is TARGET_AMT. This value is zero if the person did not crash their car. But if they did crash their car, this number will be a value greater than zero. 

Your objective is to build multiple linear regression and binary logistic regression models on the training data to predict the probability that a person will crash their car and also the amount of money it will cost if the person does crash their car. You can only use the variables given to you (or variables that you derive from the variables provided).

# 1. DATA EXPLORATION

Describe the size and the variables in the insurance training data set. Consider that too much detail will cause a manager to lose interest while too little detail will make the manager consider that you aren’t doing your job.

### Data acquisition

```{r}
all_train <- read.csv("https://raw.githubusercontent.com/miachen410/DATA621/master/HW%234/insurance_training_data.csv")

eval <- read.csv("https://raw.githubusercontent.com/miachen410/DATA621/master/HW%234/insurance-evaluation-data.csv")
```

### Data structure

There are 8161 observations and 26 variables in the training dataset.

```{r}
str(all_train)
```

From summary, we can see all the levels of the variables and where the NA's are.
```{r}
summary(all_train)
```

We want to get rid of the `$` and `,` in the numerical data and `z_` in the categorical data:
```{r}
library(dplyr)

# This function removes "$" and "," and converts factor to numeric
convert1 <- function(x) {
  y = sub("\\$", "", x)
  y = sub(",", "", y)
  y = as.numeric(y)
  return(y)
}

# This function removes "z_"
convert2 <- function(x) {
  y = sub("z_", "", x)
  return(y)
}

# Convert the columns accordingly
train_clean_prep <- all_train %>%
  mutate_at(c("INCOME", "HOME_VAL", "BLUEBOOK", "OLDCLAIM"), convert1) %>%
  mutate_at(c("MSTATUS", "SEX", "EDUCATION", "JOB", "CAR_TYPE", "URBANICITY"), convert2)

# Change TARGET_FLAG class from integer to factor
train_clean_prep$TARGET_FLAG <- as.factor(train_clean_prep$TARGET_FLAG)

# Change these variables from character to factor
train_clean_prep$MSTATUS <- as.factor(train_clean_prep$MSTATUS)
train_clean_prep$SEX <- as.factor(train_clean_prep$SEX)
train_clean_prep$EDUCATION <- as.factor(train_clean_prep$EDUCATION)
train_clean_prep$JOB <- as.factor(train_clean_prep$JOB)
train_clean_prep$CAR_TYPE <- as.factor(train_clean_prep$CAR_TYPE)
train_clean_prep$URBANICITY <- as.factor(train_clean_prep$URBANICITY)
```

Let's look at the data structure again:
```{r}
str(train_clean_prep)
```


### Visulization of the data set

Let's first look at the density plots of the numerical variables to view their shapes and distributions:

```{r}
library(reshape)
library(ggplot2)
datasub = melt(train_clean_prep)
ggplot(datasub, aes(x = value)) + 
    geom_density(fill = "blue") + 
    facet_wrap(~variable, scales = 'free') 
```



# 2. DATA PREPARATION

Describe how you have transformed the data by changing the original variables or creating new variables. If you did transform the data or create new variables, discuss why you did this.

### Missing values

There are 970 rows of data with NA values. We are going to replacing them with their median values.

```{r}
# number of rows with NA's
nrow(all_train[is.na(all_train),])
```

```{r,echo=FALSE}
library(VIM)
# List all the columns with missing values
aggr_plot <- aggr(train_clean_prep, 
                  col=c('grey','blue'), 
                  numbers=TRUE, 
                  sortVars=TRUE, 
                  labels=names(train_clean_prep), 
                  cex.axis=.7,
                  gap=3, 
                  ylab=c("Histogram of missing data","Pattern"))
```

```{r}
# Replace missing values with median
train_clean_prep <- train_clean_prep %>% mutate(
  CAR_AGE = ifelse(is.na(CAR_AGE), median(CAR_AGE, na.rm = TRUE), CAR_AGE),
  HOME_VAL = ifelse(is.na(HOME_VAL), median(HOME_VAL, na.rm = TRUE), HOME_VAL),
  YOJ = ifelse(is.na(YOJ), median(YOJ, na.rm = TRUE), YOJ),
  INCOME = ifelse(is.na(INCOME), median(INCOME, na.rm = TRUE), INCOME),
  AGE = ifelse(is.na(AGE), median(AGE, na.rm = TRUE), AGE))
```


### Log Transformation

We are going to log-transform the right skewed variables that have many "0"s.

```{r}
# Replace negative values in CAR_AGE with 0
train_clean_prep$CAR_AGE <- ifelse(train_clean_prep$CAR_AGE < 0, 0, train_clean_prep$CAR_AGE)

# Remove INDEX column
train_clean <- subset(train_clean_prep, select = -INDEX)
  
# Log-transformation
train_log <- train_clean %>% mutate(
  KIDSDRIV = log(KIDSDRIV+1),
  HOMEKIDS = log(HOMEKIDS+1),
  INCOME = log(INCOME+1),
  HOME_VAL = log(HOME_VAL+1),
  TIF = log(TIF+1),
  OLDCLAIM = log(OLDCLAIM+1),
  CLM_FREQ = log(CLM_FREQ+1),
  MVR_PTS = log(MVR_PTS+1),
  CAR_AGE = log(CAR_AGE+1)
)
```



# 3. BUILD MODELS

Using the training data set, build at least two different multiple linear regression models and three different binary logistic regression models, using different variables (or the same variables with different transformations). You may select the variables manually, use an approach such as Forward or Stepwise, use a different approach such as trees, or use a combination of techniques. Describe the techniques you used. If you manually selected a variable for inclusion into the model or exclusion into the model, indicate why this was done.

Discuss the coefficients in the models, do they make sense? For example, if a person has a lot of traffic tickets, you would reasonably expect that person to have more car crashes. If the coefficient is negative (suggesting that the person is a safer driver), then that needs to be discussed. Are you keeping the model even though it is counter intuitive? Why? The boss needs to know.


## Binary Logistic Regression Models

We first create a full model by including all the variables. 

Coefficients (+ or -) of variables with significant p-values:

* KIDSDRIV (+): When teenagers drive your car, the car is more likely to get into crashes

* INCOME (-): Rich people are less likely to get into crashes

* PARENT1/yes (+): Single parent is more likely to get into crashes

* HOME_VAL (-): Home owners tend to drive more responsibly

* MSTATUS/yes (-): Married people tend to drive more safely

* EDUCATION/bachelor, master, phd (-): More educated people tend to drive more safely

* JOB/blue collar, clerical (+): Blue collar and clerical workers are more likely to get into crashes

* JOB/manager (-): Manegements tend to drive more safely

* TRAVTIME (+): Long drives to work suggest greater risk

* CAR_USE/private (-): Private cars are being driving less than commericial cars, thus the probability of collision is lower

* BLUEBOOK (-): Unknown effect on probability of collision, but probably effect the payout if there is a crash

* TIF (-): People who have been customers for a long time are usually more safe

* CAR_TYPE/panel truck, pickup, sports car, suv, van (+): Sports car has the highest coefficient, more likely to get into a car crash

* CLM_FREQ (+): The more claims you filed in the past 5 years, the more you are likely to file in the future

* REVOKED/yes (+): If your license was revoked in the past 7 years, you probably are a more risky driver

* MVR_PTS (+): If you get lots of traffic tickets, you tend to get into more crashes

* URBANICITY/highly urban, urban (+): If you live in the city, you are more likely to get into a crash

```{r}
full_log_mod <- glm(TARGET_FLAG ~ . -TARGET_AMT, data = train_log, family = 'binomial')
summary(full_log_mod)
```


### Logistic Model 1 - Backward Selection

```{r}
library(MASS)
log_mod_1 <- full_log_mod %>% stepAIC(direction = "backward", trace = FALSE)
summary(log_mod_1)
```


## Multiple Linear Regression Models

### Linear Regression Model 1

Since we want to know the cost for if the car was in a crash, we are building the model based on the 2153 observations with known crashes. 
```{r}
train_crash <- train_log[train_log$TARGET_FLAG == 1, ]

dim(train_crash)
```

```{r}
library(corrplot)
library(corrgram)
corrplot(corrgram(train_crash), method="circle")
```

```{r}
full_linear_mod <- lm(TARGET_AMT ~ ., data = train_crash)

summary(full_linear_mod)
```



# 4. SELECT MODELS

Decide on the criteria for selecting the best multiple linear regression model and the best binary logistic regression model. Will you select models with slightly worse performance if it makes more sense or is more parsimonious? Discuss why you selected your models.

For the multiple linear regression model, will you use a metric such as Adjusted R2, RMSE, etc.? Be sure to explain how you can make inferences from the model, discuss multi-collinearity issues (if any), and discuss other relevant model output. Using the training data set, evaluate the multiple linear regression model based on (a) mean squared error, (b) R2, (c) F-statistic, and (d) residual plots. For the binary logistic regression model, will you use a metric such as log likelihood, AIC, ROC curve, etc.? Using the training data set, evaluate the binary logistic regression model based on (a) accuracy, (b) classification error rate, (c) precision, (d) sensitivity, (e) specificity, (f) F1 score, (g) AUC, and (h) confusion matrix. Make predictions using the evaluation data set.

